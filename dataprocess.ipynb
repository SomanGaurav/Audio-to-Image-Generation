{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9258d72d",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f7466128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(91254, 9)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "data = pd.read_csv(\"train.csv\")\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ac6194",
   "metadata": {},
   "source": [
    "### Creating Vggish Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13c29f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torchvggish import vggish, vggish_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "839a1c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved VGGish weights to vggish_pretrained.pth\n"
     ]
    }
   ],
   "source": [
    "\n",
    "audio_folder = \"data/rawaudio\"         \n",
    "save_file = \"data/audio_embeddings.npy\"     \n",
    "device = \"cpu\"\n",
    "model = vggish()\n",
    "model.to(device)\n",
    "model.eval()       # must contain a column \"path\"\n",
    "torch.save(model.state_dict(), \"vggish_pretrained.pth\")\n",
    "print(\"Saved VGGish weights to vggish_pretrained.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89192db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/91254 [00:00<?, ?it/s]/tmp/ipykernel_235228/1911454065.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  examples = torch.tensor(examples).float()\n",
      " 45%|████▍     | 40644/91254 [58:39<2:06:19,  6.68it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty or too short, replacing with zeros: data/rawaudio/Fq107k8_9vk_0.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 57302/91254 [1:54:35<1:26:15,  6.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty or too short, replacing with zeros: data/rawaudio/zhopcEHglEI_0.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 66675/91254 [2:25:06<53:29,  7.66it/s]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     17\u001b[39m     examples = torch.tensor(examples).float()\n\u001b[32m     19\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m         emb_out = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m)\u001b[49m      \u001b[38;5;66;03m# (N, 128)\u001b[39;00m\n\u001b[32m     21\u001b[39m         emb = emb_out.mean(dim=\u001b[32m0\u001b[39m).numpy()   \u001b[38;5;66;03m# (128,)\u001b[39;00m\n\u001b[32m     23\u001b[39m embeddings_list.append(emb)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/projects/IE643_Novelty/novelty/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/projects/IE643_Novelty/novelty/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/projects/IE643_Novelty/novelty/lib/python3.12/site-packages/torchvggish/torchvggish.py:28\u001b[39m, in \u001b[36mVGG.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m     \u001b[38;5;66;03m# Transpose the output from features to\u001b[39;00m\n\u001b[32m     31\u001b[39m     \u001b[38;5;66;03m# remain compatible with vggish embeddings\u001b[39;00m\n\u001b[32m     32\u001b[39m     x = torch.transpose(x, \u001b[32m1\u001b[39m, \u001b[32m3\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/projects/IE643_Novelty/novelty/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/projects/IE643_Novelty/novelty/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/projects/IE643_Novelty/novelty/lib/python3.12/site-packages/torch/nn/modules/container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    246\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    247\u001b[39m \u001b[33;03mRuns the forward pass.\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/projects/IE643_Novelty/novelty/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/projects/IE643_Novelty/novelty/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/projects/IE643_Novelty/novelty/lib/python3.12/site-packages/torch/nn/modules/conv.py:548\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    547\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m548\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/projects/IE643_Novelty/novelty/lib/python3.12/site-packages/torch/nn/modules/conv.py:543\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    531\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    532\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    533\u001b[39m         F.pad(\n\u001b[32m    534\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    541\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    542\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m543\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "embeddings_list = []\n",
    "files_list = []\n",
    "\n",
    "for idx, path in enumerate(tqdm(paths)):\n",
    "    if idx < 66675  : \n",
    "        continue \n",
    "    try:\n",
    "        audio, sr = librosa.load(path, sr=16000, mono=True)\n",
    "        examples = vggish_input.waveform_to_examples(audio, sr)\n",
    "\n",
    "        # If no patches (empty case), create a zero vector\n",
    "        if examples.shape[0] == 0:\n",
    "            print(\"Empty or too short, replacing with zeros:\", path)\n",
    "            emb = np.zeros(128, dtype=np.float32)\n",
    "\n",
    "        else:\n",
    "            examples = torch.tensor(examples).float()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                emb_out = model(examples)      # (N, 128)\n",
    "                emb = emb_out.numpy()   # (128,)\n",
    "\n",
    "        embeddings_list.append(emb)\n",
    "        files_list.append(path)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error processing\", path, e)\n",
    "        # Also store zero embedding for corrupted files\n",
    "        embeddings_list.append(np.zeros(128, dtype=np.float32))\n",
    "        files_list.append(path)\n",
    "        continue\n",
    "\n",
    "# Convert final embeddings list to numpy array\n",
    "embeddings_array = np.vstack(embeddings_list)\n",
    "print(\"Final embeddings shape:\", embeddings_array.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20baed1e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embeddings_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43membeddings_list\u001b[49m)\n",
      "\u001b[31mNameError\u001b[39m: name 'embeddings_list' is not defined"
     ]
    }
   ],
   "source": [
    "len(embeddings_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c16ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_array = np.vstack(embeddings_list)\n",
    "print(\"Final embeddings shape:\", embeddings_array.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d51dd0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b86c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d6581728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixing index 1417 shape: (1,)\n",
      "Fixing index 1655 shape: (1,)\n",
      "Fixing index 2809 shape: (1,)\n",
      "Fixing index 5606 shape: (1,)\n",
      "Fixing index 9450 shape: (1,)\n",
      "Fixing index 10197 shape: (1,)\n",
      "Fixing index 10835 shape: (1,)\n",
      "Fixing index 13767 shape: (1,)\n",
      "Fixing index 18371 shape: (1,)\n",
      "Fixing index 18454 shape: (1,)\n",
      "Fixing index 18626 shape: (1,)\n",
      "Fixing index 20174 shape: (1,)\n",
      "Fixing index 23426 shape: (1,)\n",
      "Fixing index 23458 shape: (1,)\n",
      "Fixing index 24988 shape: (1,)\n",
      "Fixing index 26409 shape: (1,)\n",
      "Fixing index 27723 shape: (1,)\n",
      "Fixing index 28678 shape: (1,)\n",
      "Fixing index 28794 shape: (1,)\n",
      "Fixing index 29492 shape: (1,)\n",
      "Fixing index 30061 shape: (1,)\n",
      "Fixing index 30539 shape: (1,)\n",
      "Fixing index 30914 shape: (1,)\n",
      "Fixing index 35198 shape: (1,)\n",
      "Fixing index 36746 shape: (1,)\n",
      "Fixing index 38458 shape: (1,)\n",
      "Fixing index 41850 shape: (1,)\n"
     ]
    }
   ],
   "source": [
    "clean_embeddings = []\n",
    "bad_indices = []\n",
    "\n",
    "for i, emb in enumerate(embeddings_list):\n",
    "    emb = np.array(emb)              # ensure ndarray\n",
    "    emb = emb.reshape(-1)            # flatten to 1D\n",
    "    if emb.shape[0] != 128:\n",
    "        print(\"Fixing index\", i, \"shape:\", emb.shape)\n",
    "        emb = np.zeros(128, np.float32)   # replace invalid shapes\n",
    "        bad_indices.append(i)\n",
    "    clean_embeddings.append(emb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "699a36d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final embeddings shape: (43728, 128)\n"
     ]
    }
   ],
   "source": [
    "embeddings_array = np.vstack(clean_embeddings)\n",
    "print(\"Final embeddings shape:\", embeddings_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ddc200d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved embeddings to audio_embeddings2.npy\n"
     ]
    }
   ],
   "source": [
    "np.save(\"audio_embeddings2.npy\", embeddings_array)\n",
    "\n",
    "np.save(\"audio_filenames2.npy\", np.array(files_list))\n",
    "\n",
    "print(\"Saved embeddings to\", \"audio_embeddings2.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3380fe41",
   "metadata": {},
   "source": [
    "### Final Embeds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "22baadf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "62367a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Data 1 :- (16861, 128)\n",
      "Shape of Data 2 :- (6087, 128)\n",
      "Shape of Data 3 :- (43728, 128)\n"
     ]
    }
   ],
   "source": [
    "data1 = np.load(\"audio_embeddings.npy\")\n",
    "data2 = np.load(\"audio_embeddings1.npy\")\n",
    "data3 = np.load(\"audio_embeddings2.npy\")\n",
    "\n",
    "\n",
    "print(f\"Shape of Data 1 :- {data1.shape}\")\n",
    "print(f\"Shape of Data 2 :- {data2.shape}\")\n",
    "print(f\"Shape of Data 3 :- {data3.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b315585a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of embeds (66676, 128)\n"
     ]
    }
   ],
   "source": [
    "X_embeds = np.concat([data1 , data2 , data3] , axis=0)\n",
    "print(f\"Shape of embeds {X_embeds.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "88c4d3a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of caption 1 :- (16861,)\n",
      "Shape of caption 2 :- (6087,)\n",
      "Shape of caption 3 :- (43728,)\n"
     ]
    }
   ],
   "source": [
    "caption1 = np.load(\"audio_filenames.npy\")\n",
    "caption2 = np.load(\"audio_filenames1.npy\")\n",
    "caption3 = np.load(\"audio_filenames2.npy\")\n",
    "\n",
    "print(f\"Shape of caption 1 :- {caption1.shape}\")\n",
    "print(f\"Shape of caption 2 :- {caption2.shape}\")\n",
    "print(f\"Shape of caption 3 :- {caption3.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a6241103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y captions shape :- (66676,)\n"
     ]
    }
   ],
   "source": [
    "y_filename = np.concat([caption1 , caption2 , caption3] , axis=0)\n",
    "print(f\"y captions shape :- {y_filename.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "67c21aeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>caption</th>\n",
       "      <th>caption3</th>\n",
       "      <th>caption5</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a woman laughs and speaks while birds vocalize...</td>\n",
       "      <td>woman laugh speak</td>\n",
       "      <td>woman laugh speak bird vocalize</td>\n",
       "      <td>data/rawaudio/pgLXVFvo5GI_27.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a large vehicle idling accompanied by rapid be...</td>\n",
       "      <td>large vehicle idle</td>\n",
       "      <td>large vehicle idle accompany rapid</td>\n",
       "      <td>data/rawaudio/pgN-Why-duY_0.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>several ducks quack while some liquid splashes</td>\n",
       "      <td>duck quack liquid</td>\n",
       "      <td>duck quack liquid splash splash</td>\n",
       "      <td>data/rawaudio/pgSCmHYy0eQ_320.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hissing noises from steam followed by a man ta...</td>\n",
       "      <td>hiss noise steam</td>\n",
       "      <td>hiss noise steam follow man</td>\n",
       "      <td>data/rawaudio/pgVhbq6W3Ow_30.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>an objects is being crumpled</td>\n",
       "      <td>object crumple crumple</td>\n",
       "      <td>object crumple crumple crumple crumple</td>\n",
       "      <td>data/rawaudio/pgYs-4Trnek_120.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66671</th>\n",
       "      <td>a woman is giving a speech in a foreign langua...</td>\n",
       "      <td>woman give speech</td>\n",
       "      <td>woman give speech foreign language</td>\n",
       "      <td>data/rawaudio/705wLhzLSD0_30.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66672</th>\n",
       "      <td>a train horn sounds as crossing bells ring</td>\n",
       "      <td>train horn sound</td>\n",
       "      <td>train horn sound cross bell</td>\n",
       "      <td>data/rawaudio/kllCTj91GKk_240.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66673</th>\n",
       "      <td>insects buzz while a man speaks</td>\n",
       "      <td>insect buzz man</td>\n",
       "      <td>insect buzz man speak speak</td>\n",
       "      <td>data/rawaudio/klmuMkxO6Z4_240.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66674</th>\n",
       "      <td>an engine runs and humming occurs followed by ...</td>\n",
       "      <td>engine run humming</td>\n",
       "      <td>engine run humming occur follow</td>\n",
       "      <td>data/rawaudio/klhCtuwiOHU_100.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66675</th>\n",
       "      <td>scratching occurs then a chainsaw runs cutting...</td>\n",
       "      <td>scratch occur chainsaw</td>\n",
       "      <td>scratch occur chainsaw run cut</td>\n",
       "      <td>data/rawaudio/km6wBBns7T8_240.wav</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>66676 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 caption  \\\n",
       "0      a woman laughs and speaks while birds vocalize...   \n",
       "1      a large vehicle idling accompanied by rapid be...   \n",
       "2         several ducks quack while some liquid splashes   \n",
       "3      hissing noises from steam followed by a man ta...   \n",
       "4                           an objects is being crumpled   \n",
       "...                                                  ...   \n",
       "66671  a woman is giving a speech in a foreign langua...   \n",
       "66672         a train horn sounds as crossing bells ring   \n",
       "66673                    insects buzz while a man speaks   \n",
       "66674  an engine runs and humming occurs followed by ...   \n",
       "66675  scratching occurs then a chainsaw runs cutting...   \n",
       "\n",
       "                     caption3                                caption5  \\\n",
       "0           woman laugh speak         woman laugh speak bird vocalize   \n",
       "1          large vehicle idle      large vehicle idle accompany rapid   \n",
       "2           duck quack liquid         duck quack liquid splash splash   \n",
       "3            hiss noise steam             hiss noise steam follow man   \n",
       "4      object crumple crumple  object crumple crumple crumple crumple   \n",
       "...                       ...                                     ...   \n",
       "66671       woman give speech      woman give speech foreign language   \n",
       "66672        train horn sound             train horn sound cross bell   \n",
       "66673         insect buzz man             insect buzz man speak speak   \n",
       "66674      engine run humming         engine run humming occur follow   \n",
       "66675  scratch occur chainsaw          scratch occur chainsaw run cut   \n",
       "\n",
       "                                    path  \n",
       "0       data/rawaudio/pgLXVFvo5GI_27.wav  \n",
       "1        data/rawaudio/pgN-Why-duY_0.wav  \n",
       "2      data/rawaudio/pgSCmHYy0eQ_320.wav  \n",
       "3       data/rawaudio/pgVhbq6W3Ow_30.wav  \n",
       "4      data/rawaudio/pgYs-4Trnek_120.wav  \n",
       "...                                  ...  \n",
       "66671   data/rawaudio/705wLhzLSD0_30.wav  \n",
       "66672  data/rawaudio/kllCTj91GKk_240.wav  \n",
       "66673  data/rawaudio/klmuMkxO6Z4_240.wav  \n",
       "66674  data/rawaudio/klhCtuwiOHU_100.wav  \n",
       "66675  data/rawaudio/km6wBBns7T8_240.wav  \n",
       "\n",
       "[66676 rows x 4 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[:66676 , 4:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6673d8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect count: 43728\n"
     ]
    }
   ],
   "source": [
    "incorrects = 0\n",
    "paths = data.iloc[:66676, -1].values\n",
    "\n",
    "for index, filepath in enumerate(y_filename):\n",
    "    if filepath != paths[index]:\n",
    "        incorrects += 1\n",
    "\n",
    "print(\"Incorrect count:\", incorrects)\n",
    "for index, filepath in enumerate(y_filename):\n",
    "    if filepath != paths[index]:\n",
    "        incorrects += 1\n",
    "\n",
    "print(\"Incorrect count:\", incorrects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ab80672d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_filename[0].item() == paths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e89fb0b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/rawaudio/pgLXVFvo5GI_27.wav'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_filename[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0eb07d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(66676,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# numpy file paths\n",
    "np_paths = y_filename  # shape (66676,)\n",
    "\n",
    "# dataframe containing path and caption\n",
    "df = data[['path', 'caption']]          # adjust column names if needed\n",
    "\n",
    "# create lookup dictionary\n",
    "path_to_caption = dict(zip(df['path'], df['caption']))\n",
    "\n",
    "# build captions for each numpy path\n",
    "captions = [path_to_caption.get(p, \"NO_CAPTION\") for p in np_paths]\n",
    "\n",
    "captions = np.array(captions)\n",
    "print(captions.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "27e236b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.str_('a train horn sounds as crossing bells ring'),\n",
       " np.str_('data/rawaudio/kllCTj91GKk_240.wav'))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions[66673] , y_filename[66673]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1750257f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_embeds shape :- (66676, 128)\n",
      "y_filename shape :- (66676,)\n",
      "caption shape :- (66676,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"X_embeds shape :- {X_embeds.shape}\")\n",
    "print(f\"y_filename shape :- {y_filename.shape}\")\n",
    "print(f\"caption shape :- {captions.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8c7d314e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"embeds.npy\" , X_embeds)\n",
    "np.save(\"filename.npy\" , y_filename)\n",
    "np.save(\"captions.npy\" , captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941e9286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (66676, 128)\n",
      "Filenames shape: (66676,)\n",
      "Captions shape: (66676,)\n",
      "\n",
      "Sample entries:\n",
      "Embedding vector shape: (128,)\n",
      "Filename[0]: data/rawaudio/pgLXVFvo5GI_27.wav\n",
      "Caption[0]: a woman laughs and speaks while birds vocalize and water splashes\n"
     ]
    }
   ],
   "source": [
    "# Load saved arrays\n",
    "X_embeds = np.load(\"embeds.npy\", allow_pickle=True)\n",
    "y_filename = np.load(\"filename.npy\", allow_pickle=True)\n",
    "captions = np.load(\"captions.npy\", allow_pickle=True)\n",
    "\n",
    "# Print shapes\n",
    "print(\"Embeddings shape:\", X_embeds.shape)\n",
    "print(\"Filenames shape:\", y_filename.shape)\n",
    "print(\"Captions shape:\", captions.shape)\n",
    "\n",
    "# Check they align\n",
    "print(\"\\nSample entries:\")\n",
    "print(\"Embedding vector shape:\", X_embeds[0].shape)\n",
    "print(\"Filename[0]:\", y_filename[0])\n",
    "print(\"Caption[0]:\", captions[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709647d8",
   "metadata": {},
   "source": [
    "### Creating sequence embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a26a8d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "paths = data['path'].tolist()\n",
    "random.shuffle(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dd4e11d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91254"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "113da981",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (12): ReLU(inplace=True)\n",
       "    (13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (14): ReLU(inplace=True)\n",
       "    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (embeddings): Sequential(\n",
       "    (0): Linear(in_features=12288, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): Linear(in_features=4096, out_features=128, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "audio_folder = \"data/rawaudio\"         \n",
    "save_file = \"data/audio_embeddings.npy\"     \n",
    "device = \"cpu\"\n",
    "model = vggish()\n",
    "model.to(device)\n",
    "model.eval()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b988d46b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/91254 [00:00<?, ?it/s]/tmp/ipykernel_469630/4054380676.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  examples = torch.tensor(examples).float()\n",
      "  0%|          | 76/91254 [00:16<4:23:57,  5.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing data/rawaudio/QvJIJtzTfbk_0.wav too many indices for tensor of dimension 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 999/91254 [03:11<4:39:04,  5.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing data/rawaudio/6MfLQf7E8iE_0.wav too many indices for tensor of dimension 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3069/91254 [09:53<3:54:40,  6.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing data/rawaudio/VcD4ezit_sM_0.wav too many indices for tensor of dimension 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 4155/91254 [13:27<4:00:31,  6.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing data/rawaudio/d2Ak3vmuaPE_0.wav too many indices for tensor of dimension 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 5161/91254 [16:41<3:44:20,  6.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing data/rawaudio/OmF-45c0B4E_0.wav too many indices for tensor of dimension 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 6994/91254 [22:34<4:12:56,  5.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing data/rawaudio/iE571vq9UFI_0.wav too many indices for tensor of dimension 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 7506/91254 [24:13<3:49:13,  6.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing data/rawaudio/TZLWayyX2TY_0.wav too many indices for tensor of dimension 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 8380/91254 [27:02<3:23:46,  6.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty or too short, replacing with zeros: data/rawaudio/_Ox52y_7Hoo_0.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 10001/91254 [32:08<4:21:09,  5.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final embeddings shape: (10001, 1280)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import librosa\n",
    "from tqdm import tqdm\n",
    "\n",
    "embeddings_list = []\n",
    "files_list = []\n",
    "\n",
    "TARGET_PATCHES = 10   # desired number of patches\n",
    "\n",
    "for idx, path in enumerate(tqdm(paths)):\n",
    "    if idx > 10000 : \n",
    "        break \n",
    "\n",
    "    try:\n",
    "        audio, sr = librosa.load(path, sr=16000, mono=True)\n",
    "        examples = vggish_input.waveform_to_examples(audio, sr)   # (N, 96, 64)\n",
    "\n",
    "        if examples.shape[0] == 0:\n",
    "            print(\"Empty or too short, replacing with zeros:\", path)\n",
    "            emb = np.zeros((TARGET_PATCHES, 128), dtype=np.float32)\n",
    "\n",
    "        else:\n",
    "            examples = torch.tensor(examples).float()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                emb_out = model(examples)     # (N, 128)\n",
    "\n",
    "            N = emb_out.shape[0]\n",
    "\n",
    "            # Pad or truncate to fixed length (10,128)\n",
    "            if N < TARGET_PATCHES:\n",
    "                pad = torch.zeros(TARGET_PATCHES - N, 128)\n",
    "                emb = torch.cat([emb_out, pad], dim=0)  # (10, 128)\n",
    "            elif N > TARGET_PATCHES:\n",
    "                emb = emb_out[:TARGET_PATCHES, :]       # (10, 128)\n",
    "            else:\n",
    "                emb = emb_out                            # already correct shape\n",
    "\n",
    "            emb = emb.numpy().astype(np.float32)\n",
    "\n",
    "        embeddings_list.append(emb)   # (10*128 = 1280)\n",
    "        files_list.append(path)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error processing\", path, e)\n",
    "        embeddings_list.append(np.zeros(TARGET_PATCHES * 128, dtype=np.float32))\n",
    "        files_list.append(path)\n",
    "        continue\n",
    "\n",
    "# Convert final embeddings list to numpy array\n",
    "embeddings_array = np.vstack(embeddings_list)      # shape: (num_files, 1280)\n",
    "print(\"Final embeddings shape:\", embeddings_array.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1f5465c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10001, 10, 128)\n"
     ]
    }
   ],
   "source": [
    "embeddings_array = embeddings_array.reshape(-1, 10, 128)\n",
    "print(embeddings_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "789ba223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved embeddings to token_embeddings.npy\n"
     ]
    }
   ],
   "source": [
    "np.save(\"token_embeddings.npy\", embeddings_array)\n",
    "\n",
    "np.save(\"token_files.npy\", np.array(files_list))\n",
    "\n",
    "print(\"Saved embeddings to\", \"token_embeddings.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51794e76",
   "metadata": {},
   "source": [
    "Mapping captions for time based data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "03f35f1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10001,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "filepaths = np.load(\"token_files.npy\")\n",
    "filepaths.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32d3a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10001,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "np_paths = filepaths \n",
    "df = data[['path', 'caption']]          \n",
    "path_to_caption = dict(zip(df['path'], df['caption']))\n",
    "\n",
    "# build captions for each numpy path\n",
    "captions = [path_to_caption.get(p, \"NO_CAPTION\") for p in np_paths]\n",
    "\n",
    "captions = np.array(captions)\n",
    "print(captions.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fb6889b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['a pig oinks nearby while a man speaks',\n",
       "       'a man speaks, people talk in the background',\n",
       "       'someone burps nearby while several people laugh', ...,\n",
       "       'several birds chirp in the distance with some light clicks',\n",
       "       'several frogs croaking loudly',\n",
       "       'rustling occurs while wind blows and a man speaks as birds chirp'],\n",
       "      shape=(10001,), dtype='<U206')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e92e3673",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"token_captions.npy\" , captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9992d432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(66676,)\n",
      "(66676, 128)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "captions = np.load(\"filename.npy\")\n",
    "embeds = np.load(\"embeds.npy\")\n",
    "print(captions.shape)\n",
    "print(embeds.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "novelty",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
